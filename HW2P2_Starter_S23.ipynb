{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBITN0M_LKds"
      },
      "source": [
        "# HW2P2: Face Classification and Verification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NH4P-HzLRQs"
      },
      "source": [
        "Congrats on coming to the second homework in 11785: Introduction to Deep Learning. This homework significantly longer and tougher than the previous homework. You have 2 sub-parts as outlined below. Please start early! \n",
        "\n",
        "\n",
        "*   Face Recognition: You will be writing your own CNN model to tackle the problem of classification, consisting of 7000 identities\n",
        "*   Face Verification: You use the model trained for classification to evaluate the quality of its feature embeddings, by comparing the similarity of known and unknown identities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1B_m84_cU6c"
      },
      "source": [
        "Common errors which you may face in this homeworks (because of the size of the model)\n",
        "\n",
        "\n",
        "*   CUDA Out of Memory (OOM): You can tackle this problem by (1) Reducing the batch size (2) Calling `torch.cuda.empty_cache()` and `gc.collect()` (3) Finally restarting the runtime\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instruction to Run the Code"
      ],
      "metadata": {
        "id": "Qzf6cJRUPgTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To run the final model corresponding to the highest kaggle submission, please first make sure the **Global Variables** (next section) are set to fit the purpose, and then go to **Runtime**, click **Restart and run all**. This would\n",
        "1. pip install, import and download all required packages and data\n",
        "2. run all functions and classes for loading data and creating model/optimizer/scheduler\n",
        "3. train the model for 42 epochs based on the parameters saved in the config_king variable defined under **Parameter Configuration**. \n",
        "4. Sequentially finetune the model and reset learning rates accordingly for the verification task\n",
        "\n",
        "**Note** that by default, the notebook is expected to finish running in one click. If you pause the run and want to reload the model from saved path for finetuning, please set **FINETUNE_FROM_RELOAD** as True under Global Variable section. \n",
        "\n",
        "**Note** that to run the train data using Spherenet36 model, the notebook require a GCP environment with at least 4vCPU.\n",
        "\n",
        "By default, the notebook would run the trained model on the test dataset and save the predicted result in csv file, but it would not make the submission to Kaggle. To run the notebook with kaggle submission, set **SUBMIT_KAGGLE** to True. "
      ],
      "metadata": {
        "id": "8e5RGGiZPjBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Variables"
      ],
      "metadata": {
        "id": "MterO7vlrCfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONNECT_DRIVE = False\n",
        "SELECT_MODEL = 'spherenet' # resnet, mobilenet\n",
        "USE_CELOSS = True\n",
        "FINETUNE_FROM_LOAD = False # whether the finetune is pause and need re-load\n",
        "SUBMIT_KAGGLE = False"
      ],
      "metadata": {
        "id": "65KFFGurrBlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QAiD0UyQpNul",
        "outputId": "259ff906-2575-4382-98e2-5b2e378f3a8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README"
      ],
      "metadata": {
        "id": "dWK8h4HYTv2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Score Hyperparameters:\n",
        "* **model** = SphereNet36\n",
        "* **basic convolution block** = 2d Convolutional Layer > 2d Batchnorm > PReLU activation\n",
        "* **residual model block** = 2 basic convolution block with kernel size = 3 and stride = 1, original input is added first (rather than last) to the convoluted blocks\n",
        "* **model structure** = 4 chunks of layers (each chunk is composed of one basic convolution block and a series of residual model block), a 2d Dropout layer, a Linear layer that turns the scaled output by 14x14 to lower dimension at 512, a 2d Batchnorm and a PReLU layer, and finally the output Linear layer that turns the 512 dimension into dimension of the number of classes at 7000.\n",
        "> For all chunks, the stride for initial basic convolution block is 2 and kernel size is 3. The 1st chunk has hidden size 64, 2nd 128 , 3rd 256 and 4th 512. The 1st chunk has 2 residual model blocks, the 2nd has 4, the 3rd has 8 and the 4th has 2. These numbers are referred from https://www.cvlibs.net/publications/Coors2018ECCV.pdf and https://github.com/wy1iu/SphereNet.  \n",
        "* **activation** = PReLU\n",
        "* **dropout** = 0.25\n",
        "* **weight decay** = 0.005\n",
        "* **batch size** = 64\n",
        "* **training epoch** = 42\n",
        "* **finetuning epoch** = 40 (30 for 1st level finetune and 10 for 2nd level finetune)\n",
        "* **weight init**: kaiming normal on convolution layer, constant on batchnorm layer, normal for weights and constant for biases on linear layer \n",
        "* **init model learning rate** = 0.15\n",
        "* **finetuning init model learning rate** = 0.01 for first level finetuning and 0.005 for second level finetuning\n",
        "* **training scheduler**: CosineAnnealingWarmRestarts + StepLR\n",
        "** CosineAnnealingWarmRestarts for 20 epochs x 2\n",
        "** At the 38th epoch, after stepping, switch to StepLR starting with 0.009 LR for another 3 epochs\n",
        "** StepLR for 3 epochs using gamma = 0.6 and step_size = 1\n",
        "* **finetuning scheduler**: CosineAnnealingWarmRestarts\n",
        "** CosineAnnealingWarmRestarts for 15 epochs x 2 (first level)\n",
        "** CosineAnnealingWarmRestarts for 10 epochs x 1 (second level)\n",
        "* **model base loss**: CrossEntropyLoss \n",
        "* **model base loss smoothing** = 0.1\n",
        "* **center loss weight**: 0.001 for training and 0.003 for finetuning\n",
        "* **center loss learning rate**: \n",
        "> --fixed at 0.1 for training <br>\n",
        "> --flexible (follows CosineAnnealingWarmRestarts) starting at 0.5 for first level finetuning <br>\n",
        "> --flexible (follows CosineAnnealingWarmRestarts) starting at 0.25 for second level finetuning <br>\n",
        "* **optimizer**: SGD with 0.9 momentum\n",
        "* **verification threshold**: 0.435\n",
        "\n",
        "\n",
        "\n",
        "## Data Loading and Transformation Scheme:\n",
        "The highest kaggle score is run by loading the train dataset and apply **RandomHorizontalFlip**, **RandomRotation(10)**, **ToTensor**, and **Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])** in sequence. Note that such combination comes from various ablation studies, including adding **RandomVerticalFlip**, using **RandomRotation(45)**,using **0.5** as normalize value, etc. Note that the mean and std values used in normalization refer to PyTorch ImageNet values. \n",
        "\n",
        "For test and validation data, none of the transformation is applied. However, both are normalized using the above mean and std. \n",
        "\n",
        "\n",
        "## Architectures:\n",
        "The highest kaggle score is reached using **SphereNet36** architecture. It has four chunks of SphereNet Residual blocks, follow by a dropout layer, a linear layer, a batchnorm layer, a PReLU layer, and the final classification linear layer.  \n",
        "\n",
        "1. 1st Chunk: 2 SphereNet residual blocks, hidden size is 64.\n",
        "2. 2nd Chunk: 4 SphereNet residual blocks, hidden size is 128.\n",
        "3. 3rd Chunk: 8 SphereNet residual blocks, hidden size is 256.\n",
        "4. 4th Chunk: 2 SphereNet residual blocks, hidden size is 512.\n",
        "5. Dropout Layer with 0.25 rate\n",
        "6. Linear layer (from 512 x 14 x 14 to 512) + BatchNorm + PReLU\n",
        "7. Classification Linear Layer (from 512 to 7000)\n",
        "\n",
        "Note that in the 6th layer, 512 is multiply by 14 x 14 as the image size usually are factor by 1/16 on each side in SphereNet (224/16=14), hence we multiply 14 back to get the output size from the last chunk. \n",
        "\n",
        "Other architectures are tested as well, but with less ideal performance:\n",
        "1. ResNet 34 with 4 chunks, with hidden size 64, 128, 256, 512, and block number 3, 4, 6, 3. \n",
        "2. SphereNet64 with 4 chunks, with same hidden size as above but block bumber 3, 8, 16, 3. This requires a much larger computation power with marginal improvement. \n",
        "3. MobileNetV2 with \n",
        "* initial layer with 32 hidden size and stride 2\n",
        "* 7 chunks, with expansion [1, 6, 6, 6, 6, 6, 6], channels [16, 24, 32, 64, 96, 160, 320], number of layers [1, 2, 3, 4, 3, 3, 1] and stride [1, 2, 2, 2, 1, 2, 1]\n",
        "* last layer with 1280 hidden size and stride 1\n",
        "\n",
        "\n",
        "## Epochs\n",
        "I trained the model for 42 epochs and finetuned the model for another 40 epochs to get the highest kaggle submission for both classification and verification tasks. \n",
        "\n",
        "During Training, the model learning rate is scheduled to follow CosineAnnealingWarmRestarts for 20 epochs twice, until the 38th epoch. After stepping in epoch 38th, the scheduler is switched to use StepLR, so as to approach the smaller learning rate slower for another 3 epochs. Note the center loss learning rate is fixed at 0.1.  \n",
        "\n",
        "During Finetuning, the model learning rate and center loss learning rate are scheduled to follow CosineAnnealingWarmRestarts for 15 epochs twice, and after halving the initial learning rates, finetune for another 10 epochs once. \n",
        "\n",
        "\n",
        "## Hyperparameters and Experiments\n",
        "* **Init Model Learning Rate (training)**: 0.15. I started with 0.1, and found that a slightly larger learning rate with CosineAnnealingWarmRestarts works better during ablation study. \n",
        "\n",
        "* **Batch Size**: 64. Since SphereNet 36 alreadys use 200m parameters, the batch size has to be limited to 64 when we added center loss. I used 96 for SphereNet36 + CrossEntrophyLoss, but has to reduce batch size when introducing center loss. \n",
        "\n",
        "* **Weight Decay**: 0.005. I did not try various weight decay value, but this value works well with the overall model architecture. \n",
        "\n",
        "* **Dropout**: 0.25. Though CNNs tend to not include dropout, I found my model starts to overfit at epoch 8 if not adding dropout. After adding dropout, the model does not overfit until epoch 11-12. Hence I keep this dropout layer with relatively small dropout rate. \n",
        "\n",
        "* **Init model learning rate (1st finetuning)**: 0.01. During finetuning, it is not necessary to start with a large learning rate, as it would significantly drop the val accuracy and the model needs more epochs to surpass previous performance. Hence, I round the 0.009 learning rate (at 19th or 39th epoch) to 0.01 and use it as the initial learning rate for finetuning. I tried 0.05 as well but the val accuracy is not as good as 0.01. \n",
        "\n",
        "* **Init model learning rate (2nd finetuning)**: 0.005. During the 2nd level finetuning, the model has almost reach a saturated stage, hence I half the starting learning rate in order to reach a even lower learning rate at the end using CosineAnnealingWarmRestarts. \n",
        "\n",
        "* **CrossEntropy Loss Label Smoothing**: 0.1. Adding label smoothing obviously improves model performance, and based on my search online, 0.1 seems to be a widely-used value. I did not do ablation study on this parameters.\n",
        "\n",
        "* **Center Loss Weight (training)**: 0.001. Center loss value is very large, so I used 0.001 to avoid the loass from exploding. \n",
        "* **Center Loss Weight (finetuning)**: 0.003. For finetuning, a new center loss with slightly larger weight is used. This value is inspired by the center loss paper, and 0.003 is the chosen value in the paper. \n",
        "* **Center Loss learning rate (training)**: 0.1. For training, the learning rate is fixed at 0.1, as suggested by TAs. I avoid larger learning rate as the model is still at early learning stage, and I avoid smaller learning rate as the rate is not going to change during training, and starting with a too small rate could make the training less efficient. \n",
        "* **Center Loss learning rate (1st finetuning)**: 0.5. For finetuning, I used the recommended learning rate in the center loss paper at 0.5, and let the learning rate to decrease in CosineAnnealingWarmRestarts schedule for 15 epochs loop twice. \n",
        "* **Center Loss learning rate (2nd finetuning)**: For the last 10 epoch, I half the init learning rate and let the rate to decrease in CosineAnnealing schedule for 10 epochs. \n",
        "* **Verification Threshold**: 0.435. Note that this threshold is derived by testing threshold value on validation dataset and adjust the value accordingly. 0.445 is the best threshold value for validation data, but the mean value for test data is about 0.02 smaller than validation dataset, hence I subtract 0.01 from the best threshold value and use 0.435. \n",
        "\n",
        "## Other Experiments\n",
        "\n",
        "### Scheduler \n",
        "I used ReduceLROnPlateau at first, but the model learned very slowly during training, as the learning rate is quite large and not decreasing for the early epochs. I then switched to CosineAnnealingWarmRestarts for 20 epochs and it performs much better and quicker. The 20 comes from observing the chart of ResNet34, where the validation accuracy starts to fluctuate more frequently at around 20 epochs. \n",
        "\n",
        "For Finetuning, I reduced the learning rate and hence the CosineAnnealingWarmRestarts period as well. The first two loops are 15 epochs, and the last loop is 10 epochs total. For the last 10 epochs, the learning rate is further reduce by half. Note that CosineAnnealingWarmRestarts and CosineAnnealing would be the same for the last 10 epochs. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H9vxDBX6Txbe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdoDIKWOMF59"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jza7lwiScUhb",
        "outputId": "9cca6890-2e66-4a53-d772-4a2aa8d0a8fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Apr  6 20:37:26 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # to see what GPU you have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTxfd_nqFnL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e8c0d5-9e45-4402-973b-18e35e5c4458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwLEd0gdPbSc",
        "outputId": "e7db2f98-2931-46c9-ed6e-5081bde3dca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "import torchvision #This library is used for image-based operations (Augmentations)\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import glob\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRz9et3SZnbO"
      },
      "outputs": [],
      "source": [
        "if CONNECT_DRIVE:\n",
        "    from google.colab import drive # Link your drive if you are a colab user\n",
        "    drive.mount('/content/drive') # Models in this HW take a long time to get trained and make sure to save it her"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scOnMklwWBY6"
      },
      "source": [
        "# Download Data from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BksgPdkQwwb",
        "outputId": "ce64a48d-a55b-469a-8459-21cc46cf4c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle==1.5.8\n",
            "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73272 sha256=49cae3d25be394fda287ec40bc22414d83d8066fd1265531e9e08e92ffe71d38\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/02/ef/3f8c8d86b8d5388a1d3155876837f1a1a3143ab3fc2ff1ffad\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.13\n",
            "    Uninstalling kaggle-1.5.13:\n",
            "      Successfully uninstalled kaggle-1.5.13\n",
            "Successfully installed kaggle-1.5.8\n"
          ]
        }
      ],
      "source": [
        "# TODO: Use the same Kaggle code from HW1P2\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"sharonxin1207\",\"key\":\"a6eb67109ee97e7f02df4bfe642cf615\"}') \n",
        "    # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oFjaJTaRjT7",
        "outputId": "835406ac-96d9-45f5-99d8-064b35c0054b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 11-785-s23-hw2p2-classification.zip to /content\n",
            " 99% 1.71G/1.72G [00:20<00:00, 106MB/s]\n",
            "100% 1.72G/1.72G [00:20<00:00, 91.7MB/s]\n",
            "Downloading 11-785-s23-hw2p2-verification.zip to /content\n",
            " 53% 9.00M/16.8M [00:00<00:00, 92.1MB/s]\n",
            "100% 16.8M/16.8M [00:00<00:00, 89.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir '/content/data'\n",
        "\n",
        "!kaggle competitions download -c 11-785-s23-hw2p2-classification\n",
        "!unzip -qo '11-785-s23-hw2p2-classification.zip' -d '/content/data'\n",
        "\n",
        "!kaggle competitions download -c 11-785-s23-hw2p2-verification\n",
        "!unzip -qo '11-785-s23-hw2p2-verification.zip' -d '/content/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O68hT27SXClj"
      },
      "source": [
        "# Parameter Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7qpMxG0XCJz"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'batch_size': 64, # Increase this if your GPU can handle it - 96 for spherenet only, have to reduce to 64 if using celoss\n",
        "    'lr': 0.15,\n",
        "    'celoss_lr': 0.1, \n",
        "    'celoss_weight': 0.001,\n",
        "    # use different learning rate and weight for fintune\n",
        "    'finetune': {\n",
        "        'model_lr': 0.01,\n",
        "        'celoss_lr': 0.5,\n",
        "        'celoss_weight': 0.003,\n",
        "        'epochs_1':30,\n",
        "        'epochs_2':10,\n",
        "        '2ft_lr_factor':0.1\n",
        "    },\n",
        "\n",
        "    'resnet': {\n",
        "        'epochs': 88,\n",
        "        'dropout': 0.4\n",
        "    },\n",
        "# reference: https://sahiltinky94.medium.com/know-about-mobilenet-v2-implementation-from-scratch-using-pytorch-8e589b55599\n",
        "    'mobilenet':  {\n",
        "        'bottleneck': [[1, 16, 1, 1], # t = expansion, c = channels, n=num layers, s=stride\n",
        "\t                     [6, 24, 2, 2],\n",
        "\t                     [6, 32, 3, 2],\n",
        "\t                     [6, 64, 4, 2],\n",
        "                       [6, 96, 3, 1],\n",
        "\t                     [6, 160, 3, 2],\n",
        "                       [6, 320, 1, 1]],\n",
        "        'init_layer': [0, 32, 1, 2],\n",
        "        'last_layer': [0, 1280, 1, 1],\n",
        "        'epochs': 100,\n",
        "        'dropout': 0.5\n",
        "        },\n",
        "        \n",
        "# reference:\n",
        "# https://www.cvlibs.net/publications/Coors2018ECCV.pdf\n",
        "# https://github.com/wy1iu/SphereNet  \n",
        "    'spherenet': {\n",
        "        'epochs': 42, \n",
        "        'linear_scale_factor': 1,\n",
        "        'dropout': 0.25\n",
        "    },\n",
        "\n",
        "    'num_classes': 7000,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSeiKHYrM-6b"
      },
      "source": [
        "# Classification Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmRX5omaNDEZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee1aa12-a613-44f2-a710-890676f151f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR    = '/content/data/11-785-s23-hw2p2-classification/'# TODO: Path where you have downloaded the data\n",
        "TRAIN_DIR   = os.path.join(DATA_DIR, \"train\") \n",
        "VAL_DIR     = os.path.join(DATA_DIR, \"dev\")\n",
        "TEST_DIR    = os.path.join(DATA_DIR, \"test\")\n",
        "\n",
        "# Transforms using torchvision - Refer https://pytorch.org/vision/stable/transforms.html\n",
        "\n",
        "train_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomHorizontalFlip(), \n",
        "    #torchvision.transforms.RandomVerticalFlip(),\n",
        "    torchvision.transforms.RandomRotation(10),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    # referring pytorch imagenet values: https://pytorch.org/vision/stable/models.html\n",
        "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
        "    #torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])# Implementing the right train transforms/augmentation methods is key to improving performance.\n",
        "\n",
        "# Most torchvision transforms are done on PIL images. So you convert it into a tensor at the end with ToTensor()\n",
        "# But there are some transforms which are performed after ToTensor() : e.g - Normalization\n",
        "# Normalization Tip - Do not blindly use normalization that is not suitable for this dataset\n",
        "\n",
        "valid_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset   = torchvision.datasets.ImageFolder(TRAIN_DIR, transform= train_transforms)\n",
        "valid_dataset   = torchvision.datasets.ImageFolder(VAL_DIR, transform= valid_transforms)\n",
        "# You should NOT have data augmentation on the validation set. Why?\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_dataset, \n",
        "    batch_size  = config['batch_size'], \n",
        "    shuffle     = True,\n",
        "    num_workers = 4, \n",
        "    pin_memory  = True\n",
        ")\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = valid_dataset, \n",
        "    batch_size  = config['batch_size'],\n",
        "    shuffle     = False,\n",
        "    num_workers = 2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqSR063BGE2e"
      },
      "outputs": [],
      "source": [
        "# You can do this with ImageFolder as well, but it requires some tweaking\n",
        "class ClassificationTestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir   = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in the test directory\n",
        "        self.img_paths  = list(map(lambda fname: os.path.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.transforms(Image.open(self.img_paths[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVLB41KtGC2o"
      },
      "outputs": [],
      "source": [
        "test_dataset = ClassificationTestDataset(TEST_DIR, transforms = valid_transforms) #Why are we using val_transforms for Test Data?\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = config['batch_size'], shuffle = False,\n",
        "                         drop_last = False, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4t8eU9gY0Jy",
        "outputId": "b4885f1b-dae5-4f40-ff7b-1131f6f5e3f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes    :  7000\n",
            "No. of train images  :  140000\n",
            "Shape of image       :  torch.Size([3, 224, 224])\n",
            "Batch size           :  64\n",
            "Train batches        :  2188\n",
            "Val batches          :  547\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of classes    : \", len(train_dataset.classes))\n",
        "print(\"No. of train images  : \", train_dataset.__len__())\n",
        "print(\"Shape of image       : \", train_dataset[0][0].shape)\n",
        "print(\"Batch size           : \", config['batch_size'])\n",
        "print(\"Train batches        : \", train_loader.__len__())\n",
        "print(\"Val batches          : \", valid_loader.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs2Xw_tl0IQ8"
      },
      "source": [
        "## Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIoRUzCbz85y"
      },
      "outputs": [],
      "source": [
        "VISUALIZE = False\n",
        "\n",
        "if VISUALIZE:\n",
        "    # Visualize a few images in the dataset\n",
        "    # You can write your own code, and you don't need to understand the code\n",
        "    # It is highly recommended that you visualize your data augmentation as sanity check\n",
        "\n",
        "    r, c    = [5, 5]\n",
        "    fig, ax = plt.subplots(r, c, figsize= (15, 15))\n",
        "\n",
        "    k       = 0\n",
        "    dtl     = torch.utils.data.DataLoader(\n",
        "        dataset     = torchvision.datasets.ImageFolder(TRAIN_DIR, transform= train_transforms), # dont wanna see the images with transforms\n",
        "        batch_size  = config['batch_size'], \n",
        "        shuffle     = True,\n",
        "    )\n",
        "\n",
        "    for data in dtl:\n",
        "        x, y = data\n",
        "        \n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                img = x[k].numpy().transpose(1, 2, 0)\n",
        "                ax[i, j].imshow(img)\n",
        "                ax[i, j].axis('off')\n",
        "                k+=1\n",
        "        break\n",
        "\n",
        "    del dtl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set Up Functions"
      ],
      "metadata": {
        "id": "h9666H0du9qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, torch.nn.Conv2d):\n",
        "        torch.nn.init.kaiming_normal_(m.weight.data, mode='fan_out', nonlinearity='relu')\n",
        "#       torch.nn.init.kaiming_uniform_(m.weight.data)\n",
        "    elif isinstance(m, torch.nn.BatchNorm2d):\n",
        "        torch.nn.init.constant_(m.weight.data, 1)\n",
        "        torch.nn.init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
        "        torch.nn.init.constant_(m.bias.data, 0)"
      ],
      "metadata": {
        "id": "mMW0bwz2jBws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CenterLoss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Reference:\n",
        "    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, feature_dim=512, loss_weight=config['celoss_weight']):\n",
        "        super(CenterLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.feature_dim = feature_dim\n",
        "        self.loss_w = loss_weight\n",
        "        self.centers = torch.nn.Parameter(torch.randn(self.num_classes, self.feature_dim).cuda())\n",
        "        \n",
        "    def forward(self, x, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: feature matrix with shape (batch_size, feat_dim).\n",
        "            labels: ground truth labels with shape (batch_size).\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        # x^2 + c^2 - 2xc -- consult TA for the below code \n",
        "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
        "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
        "        distmat.addmm_(x.type(torch.cuda.FloatTensor), self.centers.t().type(torch.cuda.FloatTensor), beta=1.0, alpha=-2.0)\n",
        "        \n",
        "        classes = torch.arange(self.num_classes).long()\n",
        "        classes = classes.cuda()\n",
        "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
        "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
        "\n",
        "        dist = []\n",
        "        for i in range(batch_size):\n",
        "            value = distmat[i][mask[i]]\n",
        "            value = value.clamp(min=1e-12, max=1e+12)  # for numerical stability\n",
        "            dist.append(value)\n",
        "\n",
        "        dist = torch.cat(dist)\n",
        "        loss = dist.mean()*self.loss_w\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "q6dcgQ8oIvSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqmojPaWD0H"
      },
      "source": [
        "# Very Simple Network (for Mandatory Early Submission)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny-mh_ocWIJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd65d939-946a-4c55-8242-df4f13314815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 111, 111]           1,792\n",
            "       BatchNorm2d-2         [-1, 64, 111, 111]             128\n",
            "             ReLU6-3         [-1, 64, 111, 111]               0\n",
            "            Conv2d-4          [-1, 128, 27, 27]         401,536\n",
            "       BatchNorm2d-5          [-1, 128, 27, 27]             256\n",
            "             ReLU6-6          [-1, 128, 27, 27]               0\n",
            "            Conv2d-7          [-1, 256, 13, 13]         295,168\n",
            "       BatchNorm2d-8          [-1, 256, 13, 13]             512\n",
            "             ReLU6-9          [-1, 256, 13, 13]               0\n",
            "           Conv2d-10            [-1, 512, 6, 6]       1,180,160\n",
            "      BatchNorm2d-11            [-1, 512, 6, 6]           1,024\n",
            "            ReLU6-12            [-1, 512, 6, 6]               0\n",
            "AdaptiveAvgPool2d-13            [-1, 512, 1, 1]               0\n",
            "          Flatten-14                  [-1, 512]               0\n",
            "           Linear-15                 [-1, 7000]       3,591,000\n",
            "================================================================\n",
            "Total params: 5,471,576\n",
            "Trainable params: 5,471,576\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 21.66\n",
            "Params size (MB): 20.87\n",
            "Estimated Total Size (MB): 43.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class BasicNetwork(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    The Very Low early deadline architecture is a 4-layer CNN.\n",
        "\n",
        "    The first Conv layer has 64 channels, kernel size 7, and stride 4.\n",
        "    The next three have 128, 256, and 512 channels. Each have kernel size 3 and stride 2.\n",
        "    \n",
        "    Think about strided convolutions from the lecture, as convolutioin with stride= 1 and downsampling.\n",
        "    For stride 1 convolution, what padding do you need for preserving the spatial resolution? \n",
        "    (Hint => padding = kernel_size // 2) - Why?)\n",
        "\n",
        "    Each Conv layer is accompanied by a Batchnorm and ReLU layer.\n",
        "    Finally, you want to average pool over the spatial dimensions to reduce them to 1 x 1. Use AdaptiveAvgPool2d.\n",
        "    Then, remove (Flatten?) these trivial 1x1 dimensions away.\n",
        "    Look through https://pytorch.org/docs/stable/nn.html \n",
        "\n",
        "    Why does a very simple network have 4 convolutions?\n",
        "    Input images are 224x224. Note that each of these convolutions downsample.\n",
        "    Downsampling 2x effectively doubles the receptive field, increasing the spatial\n",
        "    region each pixel extracts features from. Downsampling 32x is standard\n",
        "    for most image models.\n",
        "\n",
        "    Why does a very simple network have high channel sizes?\n",
        "    Every time you downsample 2x, you do 4x less computation (at same channel size).\n",
        "    To maintain the same level of computation, you 2x increase # of channels, which \n",
        "    increases computation by 4x. So, balances out to same computation.\n",
        "    Another intuition is - as you downsample, you lose spatial information. We want\n",
        "    to preserve some of it in the channel dimension.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=7000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=2),\n",
        "            torch.nn.BatchNorm2d(64),\n",
        "            torch.nn.ReLU6(inplace=True),\n",
        "            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=7, stride=4),\n",
        "            torch.nn.BatchNorm2d(128),\n",
        "            torch.nn.ReLU6(inplace=True),\n",
        "            torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2),  \n",
        "            torch.nn.BatchNorm2d(256),\n",
        "            torch.nn.ReLU6(inplace=True),\n",
        "            torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2),\n",
        "            torch.nn.BatchNorm2d(512),\n",
        "            torch.nn.ReLU6(inplace=True),\n",
        "            torch.nn.AdaptiveAvgPool2d(output_size=(1, 1)), \n",
        "            torch.nn.Flatten()            \n",
        "            ) \n",
        "        \n",
        "        self.cls_layer = torch.nn.Linear(512, num_classes)\n",
        "    \n",
        "    def forward(self, x, return_feats=False):\n",
        "        \"\"\"\n",
        "        What is return_feats? It essentially returns the second-to-last-layer\n",
        "        features of a given image. It's a \"feature encoding\" of the input image,\n",
        "        and you can use it for the verification task. You would use the outputs\n",
        "        of the final classification layer for the classification task.\n",
        "\n",
        "        You might also find that the classification outputs are sometimes better\n",
        "        for verification too - try both.\n",
        "        \"\"\"\n",
        "        feats = self.backbone(x)\n",
        "        out = self.cls_layer(feats)\n",
        "\n",
        "        if return_feats:\n",
        "            return feats\n",
        "        else:\n",
        "            return out\n",
        "            \n",
        "model = BasicNetwork().to(DEVICE)\n",
        "summary(model, (3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SphereNet"
      ],
      "metadata": {
        "id": "kYZAPgHlr2SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlockPReLU(torch.nn.Sequential):\n",
        "    def __init__(self, in_chan, out_chan, kernel, stride, padding=-1, groups=1, bias=False):\n",
        "        if padding < 0:\n",
        "          padding = (kernel-1)//2\n",
        "        super(ConvBlockPReLU, self).__init__(\n",
        "            torch.nn.Conv2d(in_channels=in_chan, out_channels=out_chan, \n",
        "                            kernel_size=kernel, stride=stride, padding=padding,\n",
        "                            groups=groups, bias=bias),\n",
        "            torch.nn.BatchNorm2d(out_chan),\n",
        "            torch.nn.PReLU(out_chan)\n",
        "        )\n",
        "\n",
        "class SphereNetBlock(torch.nn.Module):\n",
        "    def __init__(self, in_chan):\n",
        "      super(SphereNetBlock, self).__init__()\n",
        "      layer1 = ConvBlockPReLU(in_chan, in_chan, 3, 1)\n",
        "      layer2 = ConvBlockPReLU(in_chan, in_chan, 3, 1)\n",
        "\n",
        "      self.net = torch.nn.Sequential(*[layer1, layer2])\n",
        "\n",
        "    def forward(self, x):\n",
        "            return x + self.net(x)\n",
        "\n",
        "\n",
        "class SphereNet(torch.nn.Module):\n",
        "    # [3, 7, 16, 3] for 64, [2, 4, 8, 2] for 36\n",
        "    def __init__(self, in_chan, layer_list, num_classes, block):\n",
        "        super(SphereNet, self).__init__()\n",
        "        self.first_chan = in_chan\n",
        "        self.layer1 = self.make_layer(block, layer_list[0], out_chan=64, stride=2)\n",
        "        self.layer2 = self.make_layer(block, layer_list[1], out_chan=128, stride=2)\n",
        "        self.layer3 = self.make_layer(block, layer_list[2], out_chan=256, stride=2)\n",
        "        self.layer4 = self.make_layer(block, layer_list[3], out_chan=512, stride=2)\n",
        "        \n",
        "        #self.avgpool_layer = torch.nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.dropout_layer = torch.nn.Dropout2d(0.25)\n",
        "        # image size /16 = 14 or /32 = 7\n",
        "        self.final_factor = config['spherenet']['linear_scale_factor']\n",
        "        scale_out = int(512*self.final_factor)\n",
        "        self.final_linear = torch.nn.Linear(512*14*14, scale_out)\n",
        "        self.final_batchnorm = torch.nn.BatchNorm1d(scale_out)\n",
        "        self.final_prelu = torch.nn.PReLU(scale_out)\n",
        "        self.cls_layer = torch.nn.Linear(scale_out, num_classes)\n",
        "\n",
        "      \n",
        "    def make_layer(self, block, block_num, out_chan, stride):\n",
        "        init_layer = ConvBlockPReLU(self.first_chan, out_chan, 3, stride)\n",
        "        layers = [init_layer]\n",
        "\n",
        "        self.first_chan = out_chan\n",
        "        \n",
        "        for i in range(block_num):\n",
        "            layers.append(block(out_chan))\n",
        "\n",
        "        return torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, return_feats=False):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        # x = self.avgpool_layer(x)\n",
        "        x = self.dropout_layer(x)\n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.final_linear(x)\n",
        "        feats = self.final_prelu(self.final_batchnorm(x))\n",
        "        out = self.cls_layer(feats)\n",
        "        \n",
        "\n",
        "        if return_feats:\n",
        "            return feats, out\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "def SphereNet36(num_classes):\n",
        "    # in channels and layers are fixed for this model\n",
        "    return SphereNet(3, [2, 4, 8, 2], num_classes, SphereNetBlock)\n",
        "\n",
        "def SphereNet64(num_classes):\n",
        "    # in channels and layers are fixed for this model\n",
        "    return SphereNet(3, [3, 8, 16, 3], num_classes, SphereNetBlock)\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "M5zcNkS-r4nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if SELECT_MODEL == 'spherenet':\n",
        "    model = SphereNet36(config['num_classes']).to(DEVICE)\n",
        "    model.apply(init_weights)\n",
        "    summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "eAhCuaCYaPTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e13a04-804d-434e-ae3a-7a2292fe6f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           1,728\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "             PReLU-3         [-1, 64, 112, 112]              64\n",
            "            Conv2d-4         [-1, 64, 112, 112]          36,864\n",
            "       BatchNorm2d-5         [-1, 64, 112, 112]             128\n",
            "             PReLU-6         [-1, 64, 112, 112]              64\n",
            "            Conv2d-7         [-1, 64, 112, 112]          36,864\n",
            "       BatchNorm2d-8         [-1, 64, 112, 112]             128\n",
            "             PReLU-9         [-1, 64, 112, 112]              64\n",
            "   SphereNetBlock-10         [-1, 64, 112, 112]               0\n",
            "           Conv2d-11         [-1, 64, 112, 112]          36,864\n",
            "      BatchNorm2d-12         [-1, 64, 112, 112]             128\n",
            "            PReLU-13         [-1, 64, 112, 112]              64\n",
            "           Conv2d-14         [-1, 64, 112, 112]          36,864\n",
            "      BatchNorm2d-15         [-1, 64, 112, 112]             128\n",
            "            PReLU-16         [-1, 64, 112, 112]              64\n",
            "   SphereNetBlock-17         [-1, 64, 112, 112]               0\n",
            "           Conv2d-18          [-1, 128, 56, 56]          73,728\n",
            "      BatchNorm2d-19          [-1, 128, 56, 56]             256\n",
            "            PReLU-20          [-1, 128, 56, 56]             128\n",
            "           Conv2d-21          [-1, 128, 56, 56]         147,456\n",
            "      BatchNorm2d-22          [-1, 128, 56, 56]             256\n",
            "            PReLU-23          [-1, 128, 56, 56]             128\n",
            "           Conv2d-24          [-1, 128, 56, 56]         147,456\n",
            "      BatchNorm2d-25          [-1, 128, 56, 56]             256\n",
            "            PReLU-26          [-1, 128, 56, 56]             128\n",
            "   SphereNetBlock-27          [-1, 128, 56, 56]               0\n",
            "           Conv2d-28          [-1, 128, 56, 56]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 56, 56]             256\n",
            "            PReLU-30          [-1, 128, 56, 56]             128\n",
            "           Conv2d-31          [-1, 128, 56, 56]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 56, 56]             256\n",
            "            PReLU-33          [-1, 128, 56, 56]             128\n",
            "   SphereNetBlock-34          [-1, 128, 56, 56]               0\n",
            "           Conv2d-35          [-1, 128, 56, 56]         147,456\n",
            "      BatchNorm2d-36          [-1, 128, 56, 56]             256\n",
            "            PReLU-37          [-1, 128, 56, 56]             128\n",
            "           Conv2d-38          [-1, 128, 56, 56]         147,456\n",
            "      BatchNorm2d-39          [-1, 128, 56, 56]             256\n",
            "            PReLU-40          [-1, 128, 56, 56]             128\n",
            "   SphereNetBlock-41          [-1, 128, 56, 56]               0\n",
            "           Conv2d-42          [-1, 128, 56, 56]         147,456\n",
            "      BatchNorm2d-43          [-1, 128, 56, 56]             256\n",
            "            PReLU-44          [-1, 128, 56, 56]             128\n",
            "           Conv2d-45          [-1, 128, 56, 56]         147,456\n",
            "      BatchNorm2d-46          [-1, 128, 56, 56]             256\n",
            "            PReLU-47          [-1, 128, 56, 56]             128\n",
            "   SphereNetBlock-48          [-1, 128, 56, 56]               0\n",
            "           Conv2d-49          [-1, 256, 28, 28]         294,912\n",
            "      BatchNorm2d-50          [-1, 256, 28, 28]             512\n",
            "            PReLU-51          [-1, 256, 28, 28]             256\n",
            "           Conv2d-52          [-1, 256, 28, 28]         589,824\n",
            "      BatchNorm2d-53          [-1, 256, 28, 28]             512\n",
            "            PReLU-54          [-1, 256, 28, 28]             256\n",
            "           Conv2d-55          [-1, 256, 28, 28]         589,824\n",
            "      BatchNorm2d-56          [-1, 256, 28, 28]             512\n",
            "            PReLU-57          [-1, 256, 28, 28]             256\n",
            "   SphereNetBlock-58          [-1, 256, 28, 28]               0\n",
            "           Conv2d-59          [-1, 256, 28, 28]         589,824\n",
            "      BatchNorm2d-60          [-1, 256, 28, 28]             512\n",
            "            PReLU-61          [-1, 256, 28, 28]             256\n",
            "           Conv2d-62          [-1, 256, 28, 28]         589,824\n",
            "      BatchNorm2d-63          [-1, 256, 28, 28]             512\n",
            "            PReLU-64          [-1, 256, 28, 28]             256\n",
            "   SphereNetBlock-65          [-1, 256, 28, 28]               0\n",
            "           Conv2d-66          [-1, 256, 28, 28]         589,824\n",
            "      BatchNorm2d-67          [-1, 256, 28, 28]             512\n",
            "            PReLU-68          [-1, 256, 28, 28]             256\n",
            "           Conv2d-69          [-1, 256, 28, 28]         589,824\n",
            "      BatchNorm2d-70          [-1, 256, 28, 28]             512\n",
            "            PReLU-71          [-1, 256, 28, 28]             256\n",
            "   SphereNetBlock-72          [-1, 256, 28, 28]               0\n",
            "           Conv2d-73          [-1, 256, 28, 28]         589,824\n",
            "      BatchNorm2d-74          [-1, 256, 28, 28]             512\n",
            "            PReLU-75          [-1, 256, 28, 28]             256\n",
            "           Conv2d-76          [-1, 256, 28, 28]         589,824\n",
            "      BatchNorm2d-77          [-1, 256, 28, 28]             512\n",
            "            PReLU-78          [-1, 256, 28, 28]             256\n",
            "   SphereNetBlock-79          [-1, 256, 28, 28]               0\n",
            "           Conv2d-80          [-1, 256, 28, 28]         589,824\n",
            "      BatchNorm2d-81          [-1, 256, 28, 28]             512\n",
            "            PReLU-82          [-1, 256, 28, 28]             256\n",
            "           Conv2d-83          [-1, 256, 28, 28]         589,824\n",
            "      BatchNorm2d-84          [-1, 256, 28, 28]             512\n",
            "            PReLU-85          [-1, 256, 28, 28]             256\n",
            "   SphereNetBlock-86          [-1, 256, 28, 28]               0\n",
            "           Conv2d-87          [-1, 256, 28, 28]         589,824\n",
            "      BatchNorm2d-88          [-1, 256, 28, 28]             512\n",
            "            PReLU-89          [-1, 256, 28, 28]             256\n",
            "           Conv2d-90          [-1, 256, 28, 28]         589,824\n",
            "      BatchNorm2d-91          [-1, 256, 28, 28]             512\n",
            "            PReLU-92          [-1, 256, 28, 28]             256\n",
            "   SphereNetBlock-93          [-1, 256, 28, 28]               0\n",
            "           Conv2d-94          [-1, 256, 28, 28]         589,824\n",
            "      BatchNorm2d-95          [-1, 256, 28, 28]             512\n",
            "            PReLU-96          [-1, 256, 28, 28]             256\n",
            "           Conv2d-97          [-1, 256, 28, 28]         589,824\n",
            "      BatchNorm2d-98          [-1, 256, 28, 28]             512\n",
            "            PReLU-99          [-1, 256, 28, 28]             256\n",
            "  SphereNetBlock-100          [-1, 256, 28, 28]               0\n",
            "          Conv2d-101          [-1, 256, 28, 28]         589,824\n",
            "     BatchNorm2d-102          [-1, 256, 28, 28]             512\n",
            "           PReLU-103          [-1, 256, 28, 28]             256\n",
            "          Conv2d-104          [-1, 256, 28, 28]         589,824\n",
            "     BatchNorm2d-105          [-1, 256, 28, 28]             512\n",
            "           PReLU-106          [-1, 256, 28, 28]             256\n",
            "  SphereNetBlock-107          [-1, 256, 28, 28]               0\n",
            "          Conv2d-108          [-1, 512, 14, 14]       1,179,648\n",
            "     BatchNorm2d-109          [-1, 512, 14, 14]           1,024\n",
            "           PReLU-110          [-1, 512, 14, 14]             512\n",
            "          Conv2d-111          [-1, 512, 14, 14]       2,359,296\n",
            "     BatchNorm2d-112          [-1, 512, 14, 14]           1,024\n",
            "           PReLU-113          [-1, 512, 14, 14]             512\n",
            "          Conv2d-114          [-1, 512, 14, 14]       2,359,296\n",
            "     BatchNorm2d-115          [-1, 512, 14, 14]           1,024\n",
            "           PReLU-116          [-1, 512, 14, 14]             512\n",
            "  SphereNetBlock-117          [-1, 512, 14, 14]               0\n",
            "          Conv2d-118          [-1, 512, 14, 14]       2,359,296\n",
            "     BatchNorm2d-119          [-1, 512, 14, 14]           1,024\n",
            "           PReLU-120          [-1, 512, 14, 14]             512\n",
            "          Conv2d-121          [-1, 512, 14, 14]       2,359,296\n",
            "     BatchNorm2d-122          [-1, 512, 14, 14]           1,024\n",
            "           PReLU-123          [-1, 512, 14, 14]             512\n",
            "  SphereNetBlock-124          [-1, 512, 14, 14]               0\n",
            "       Dropout2d-125          [-1, 512, 14, 14]               0\n",
            "          Linear-126                  [-1, 512]      51,380,736\n",
            "     BatchNorm1d-127                  [-1, 512]           1,024\n",
            "           PReLU-128                  [-1, 512]             512\n",
            "          Linear-129                 [-1, 7000]       3,591,000\n",
            "================================================================\n",
            "Total params: 76,749,912\n",
            "Trainable params: 76,749,912\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 303.25\n",
            "Params size (MB): 292.78\n",
            "Estimated Total Size (MB): 596.60\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet"
      ],
      "metadata": {
        "id": "zcESDXMoQuur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(torch.nn.Sequential):\n",
        "    def __init__(self, in_chan, out_chan, kernel, stride, padding=-1, groups=1, bias=False):\n",
        "        if padding < 0:\n",
        "          padding = (kernel-1)//2\n",
        "        super(ConvBlock, self).__init__(\n",
        "            torch.nn.Conv2d(in_channels=in_chan, out_channels=out_chan, \n",
        "                            kernel_size=kernel, stride=stride, padding=padding,\n",
        "                            groups=groups, bias=bias),\n",
        "            torch.nn.BatchNorm2d(out_chan),\n",
        "            torch.nn.ReLU6(inplace=True)\n",
        "        )\n",
        "\n",
        "class ResNetBlock(torch.nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_chan, out_chan, stride=1):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.downsample = self.stride == 1 and in_chan == (out_chan*self.expansion)\n",
        "        layer1 = ConvBlock(in_chan, out_chan, 3, self.stride)\n",
        "        layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(out_chan, out_chan, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm2d(out_chan)\n",
        "        )\n",
        "\n",
        "        self.basicnet = torch.nn.Sequential(*[layer1, layer2])\n",
        "\n",
        "        if not self.downsample:\n",
        "            self.ds_layer = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_chan, out_chan*self.expansion, 1, self.stride, bias=False),\n",
        "                torch.nn.BatchNorm2d(out_chan*self.expansion)\n",
        "            )\n",
        "        else:\n",
        "            self.ds_layer = torch.nn.Sequential()\n",
        "\n",
        "        self.activate = torch.nn.ReLU6()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.basicnet(x)\n",
        "        # to skip the connection if downsample condition is not met\n",
        "        out += self.ds_layer(x)\n",
        "        out = torch.nn.ReLU6(inplace=True)(out)\n",
        "        return out\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "8jZRYFcJQuOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ResNet(torch.nn.Module):\n",
        "    def __init__(self, in_chan, layer_list, num_classes, block):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.first_chan = 64\n",
        "        self.init_layer = ConvBlock(in_chan, self.first_chan, 7, 2)\n",
        "        self.maxpool_layer = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        # 64, 128, 256, 512\n",
        "        self.layer1 = self.make_layer(block, layer_list[0], out_chan=64, stride=1)\n",
        "        self.layer2 = self.make_layer(block, layer_list[1], out_chan=128, stride=2)\n",
        "        self.layer3 = self.make_layer(block, layer_list[2], out_chan=256, stride=2)\n",
        "        self.layer4 = self.make_layer(block, layer_list[3], out_chan=512, stride=2)\n",
        "        \n",
        "        self.avgpool_layer = torch.nn.AdaptiveAvgPool2d((1,1))\n",
        "        #self.dropout_layer = torch.nn.Dropout2d(config['resnet']['dropout'])\n",
        "        self.cls_layer = torch.nn.Linear(512*block.expansion, num_classes)\n",
        "        \n",
        "    def make_layer(self, block, block_num, out_chan, stride):\n",
        "        init_layer = block(self.first_chan, out_chan, stride)\n",
        "        # update input channel\n",
        "        self.first_chan = out_chan*block.expansion\n",
        "\n",
        "        layers = [init_layer]\n",
        "        \n",
        "        for i in range(block_num-1):\n",
        "            layers.append(block(self.first_chan, out_chan, 1))\n",
        "            self.first_chan = out_chan*block.expansion\n",
        "\n",
        "        return torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, return_feats=False):\n",
        "        x = self.init_layer(x)\n",
        "        x = self.maxpool_layer(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        x = self.avgpool_layer(x)\n",
        "        #x = self.dropout_layer(x)\n",
        "        # flatten\n",
        "        feats = x.view(x.size(0), -1)\n",
        "        out = self.cls_layer(feats)\n",
        "\n",
        "        if return_feats:\n",
        "            return feats, out\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "def ResNet34(num_classes):\n",
        "    # in channels and layers are fixed for this model\n",
        "    return ResNet(3, [3,4,6,3], num_classes, ResNetBlock)\n"
      ],
      "metadata": {
        "id": "Px1gfXGYSA_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if SELECT_MODEL == 'resnet':\n",
        "  model = ResNet34(config['num_classes']).to(DEVICE)\n",
        "  model.apply(init_weights)\n",
        "  summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "GXG_H0GcUKam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-sOpBHQ6-Vh"
      },
      "source": [
        "#MobileNet Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlqJlGFbCqDJ"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(torch.nn.Sequential):\n",
        "    def __init__(self, in_chan, out_chan, kernel, stride, groups=1, bias=False):\n",
        "        super(ConvBlock, self).__init__(\n",
        "            torch.nn.Conv2d(in_channels=in_chan, out_channels=out_chan, \n",
        "                            kernel_size=kernel, stride=stride, padding=(kernel-1)//2,\n",
        "                            groups=groups, bias=bias),\n",
        "            torch.nn.BatchNorm2d(out_chan),\n",
        "            torch.nn.ReLU6(inplace=True)\n",
        "        )\n",
        "\n",
        "\n",
        "class BottleNeck(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Bottleneck block fo MobileNetV2\n",
        "    \"\"\"\n",
        "    def __init__(self, in_chan, out_chan, stride, expand=1):\n",
        "        super(BottleNeck, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.skip = False\n",
        "        if self.stride == 1 and in_chan == out_chan:\n",
        "          self.skip = True\n",
        "        # expand dimension by expansion factor \n",
        "        h_dim = int(expand*in_chan)\n",
        "        # pointwise with kernel = 1 and stride = 1 and no padding\n",
        "        layer1 = ConvBlock(in_chan, h_dim, 1, 1)\n",
        "        # depthwise with kernel = 3 and stride by bottlenck and 1 padding\n",
        "        layer2 = ConvBlock(h_dim, h_dim, 3, self.stride, groups=h_dim)\n",
        "        # projection with kernal = 1 and stride = 1 and no padding\n",
        "        layer3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(h_dim, out_chan, 1, 1),\n",
        "            torch.nn.BatchNorm2d(out_chan)\n",
        "        )\n",
        "\n",
        "        self.bottlenet = torch.nn.Sequential(*[layer1, layer2, layer3]\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # skip if in and out channel is the same and stride is 1\n",
        "        if self.skip:\n",
        "            return x + self.bottlenet(x)\n",
        "        else:\n",
        "            return self.bottlenet(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pymwq9DB7FyY"
      },
      "outputs": [],
      "source": [
        "class MobileNetV2(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(self, in_chan, num_classes, bottles, blocktype=BottleNeck):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        bottle_in_chan = bottles['init_layer'][1]\n",
        "        bottle_in_stride = bottles['init_layer'][3]\n",
        "        bottle_out_chan = bottles['last_layer'][1]\n",
        "        bottle_out_stride = bottles['last_layer'][3]\n",
        "        # first layer\n",
        "        init_layer = ConvBlock(in_chan, bottle_in_chan, 3, bottle_in_stride)\n",
        "\n",
        "        blocks = [init_layer]\n",
        "        # adding bottleneck layers\n",
        "        for expand, chan, num, stride in bottles[\"bottleneck\"]:\n",
        "          print(expand, chan, num, stride)\n",
        "          blocks.append(BottleNeck(bottle_in_chan, chan, stride, expand))\n",
        "          bottle_in_chan = chan\n",
        "          if num > 1:\n",
        "              for l in range(1, num):\n",
        "                blocks.append(BottleNeck(bottle_in_chan, chan, 1, expand))\n",
        "        # last layer\n",
        "        blocks.append(ConvBlock(bottle_in_chan, bottle_out_chan, 1, bottle_out_stride))\n",
        "\n",
        "        # pooling layer and flatten\n",
        "        blocks.append(torch.nn.AdaptiveAvgPool2d(output_size=(1, 1)))\n",
        "        #blocks.append(torch.nn.Flatten())\n",
        "\n",
        "        blocks.append(torch.nn.Dropout2d(config['mobilenet']['dropout']))\n",
        "        blocks.append(torch.nn.Flatten())\n",
        "\n",
        "        self.mobilenet = torch.nn.Sequential(*blocks)\n",
        "\n",
        "        # classifier\n",
        "        \n",
        "        self.cls_layer = torch.nn.Linear(bottle_out_chan, num_classes)\n",
        "    \n",
        "    def forward(self, x, return_feats=False):\n",
        "        \"\"\"\n",
        "        What is return_feats? It essentially returns the second-to-last-layer\n",
        "        features of a given image. It's a \"feature encoding\" of the input image,\n",
        "        and you can use it for the verification task. You would use the outputs\n",
        "        of the final classification layer for the classification task.\n",
        "\n",
        "        You might also find that the classification outputs are sometimes better\n",
        "        for verification too - try both.\n",
        "        \"\"\"\n",
        "        feats = self.mobilenet(x)\n",
        "        out = self.cls_layer(feats)\n",
        "\n",
        "        if return_feats:\n",
        "            return feats, out\n",
        "        else:\n",
        "            return out\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SELECT_MODEL == 'mobilenet':\n",
        "  model = MobileNetV2(3, config['num_classes'], config['mobilenet_setting']).to(DEVICE)\n",
        "  model.apply(init_weights)\n",
        "  summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "rS9Ibs3LKF_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZCn0qHuZRKj"
      },
      "source": [
        "# Other Setup - Loss, Optimizer, Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UowI9OcUYPjP"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # TODO: What loss do you need for a multi class classification problem?\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "if USE_CELOSS:\n",
        "  finetune_loss = CenterLoss(7000, 512, config['celoss_weight'])\n",
        "  finetune_optimizer = torch.optim.SGD(finetune_loss.parameters(), lr = config['celoss_lr']) \n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 20, 1)\n",
        "#loss_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(finetune_optimizer, 24, 1)\n",
        "scaler = torch.cuda.amp.GradScaler() # Good news. We have FP16 (Mixed precision training) implemented for you\n",
        "# It is useful only in the case of compatible GPUs such as T4/V100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzM11HtcboYv"
      },
      "source": [
        "# Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_celoss(model, dataloader, optimizer, finetune_optimizer, criterion, \n",
        "          finetune_loss, loss_weight, scheduler=None):\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    # Progress Bar \n",
        "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5) \n",
        "\n",
        "    num_correct = 0\n",
        "    total_loss  = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        finetune_optimizer.zero_grad()\n",
        "\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            feats, outputs = model(images, return_feats=True)\n",
        "            loss0 = criterion(outputs, labels) # TODO: calculate cross entropy loss from outputs and labels\n",
        "            loss1 = finetune_loss(feats, labels)# TODO: calculate weighted finetune_loss (center loss) from feats and labels\n",
        "       \n",
        "        # Update no. of correct predictions & loss as we iterate\n",
        "        num_correct     += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss      += float(loss0.item())+float(loss1.item())\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            acc         = \"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss        = \"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct = num_correct,\n",
        "            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n",
        "            losslr          = \"{:.04f}\".format(float(finetune_optimizer.param_groups[0]['lr'])),\n",
        "        )       \n",
        "        # TODO: backward loss0 to calculate gradients for model paramters\n",
        "        scaler.scale(loss0).backward(retain_graph=True)\n",
        "        # Hint: You have to pass retain_graph=True here, so that the scaler will remember this backward call\n",
        "        scaler.scale(loss1).backward()\n",
        "        # TODO: backward loss1 to calculate gradients for finetune_loss paramters\n",
        "\n",
        "        # update fine tuning loss' parameters\n",
        "        # the paramerters should be adjusted according to the loss_weight you choose\n",
        "        for parameter in finetune_loss.parameters():\n",
        "            parameter.grad.data *= (1.0 / loss_weight)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.step(finetune_optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if scheduler is not None:\n",
        "           scheduler.step()\n",
        "        # if you use a scheduler to schedule your learning rate for Center Loss\n",
        "        # scheduler_finetune_loss.step()\n",
        "        \n",
        "        del images, labels, outputs, loss0, loss1\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    acc         = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss  = float(total_loss / len(dataloader))\n",
        "\n",
        "    return acc, total_loss"
      ],
      "metadata": {
        "id": "4h9pcZ2m-AD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgSw6iJJavBZ"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    # Progress Bar \n",
        "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5) \n",
        "\n",
        "    num_correct = 0\n",
        "    total_loss  = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        \n",
        "        optimizer.zero_grad() # Zero gradients\n",
        "\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        \n",
        "        with torch.cuda.amp.autocast(): # This implements mixed precision. Thats it! \n",
        "            outputs = model(images)\n",
        "            loss    = criterion(outputs, labels)\n",
        "\n",
        "        # Update no. of correct predictions & loss as we iterate\n",
        "        num_correct     += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss      += float(loss.item())\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            acc         = \"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss        = \"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct = num_correct,\n",
        "            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr']))\n",
        "        )\n",
        "        \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() \n",
        "\n",
        "        # TODO? Depending on your choice of scheduler,\n",
        "        # You may want to call some schdulers inside the train function. What are these?\n",
        "      \n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    acc         = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss  = float(total_loss / len(dataloader))\n",
        "\n",
        "    return acc, total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5V2UdnpdEoK"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader, criterion):\n",
        "  \n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
        "\n",
        "    num_correct = 0.0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        \n",
        "        # Move images to device\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        \n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct)\n",
        "\n",
        "        batch_bar.update()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss = float(total_loss / len(dataloader))\n",
        "    return acc, total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmotca6pcLLY"
      },
      "outputs": [],
      "source": [
        "gc.collect() # These commands help you when you face CUDA OOM error\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mBgKGkXLrdJ"
      },
      "source": [
        "# Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix62_BkaLr_D",
        "outputId": "3a473ede-69dc-414f-d7e1-a9a0638762b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "wandb.login(key=\"27ad915a9386068b1fc160cd97b84be7ba1fe659\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG0vmsmbRYEi",
        "outputId": "276c3ce6-2cd3-44c6-cc64-c0b855b5db62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwenxinz3\u001b[0m (\u001b[33msharonxin1207\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230406_203912-zld2p5fv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sharonxin1207/hw2p2-ablations/runs/zld2p5fv' target=\"_blank\">spherenet-run</a></strong> to <a href='https://wandb.ai/sharonxin1207/hw2p2-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sharonxin1207/hw2p2-ablations' target=\"_blank\">https://wandb.ai/sharonxin1207/hw2p2-ablations</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sharonxin1207/hw2p2-ablations/runs/zld2p5fv' target=\"_blank\">https://wandb.ai/sharonxin1207/hw2p2-ablations/runs/zld2p5fv</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name = \"spherenet-run\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw2p2-ablations\", ### Project should be created in your wandb account \n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQkRw1FvLqYe"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the threshold epoch for scheduler switch\n",
        "if SELECT_MODEL in ['resnet', 'mobilenet']:\n",
        "  switch_epoch = 77\n",
        "elif SELECT_MODEL in ['spherenet']:\n",
        "  switch_epoch = 37 # leave 5 epochs for smoothing the learning rate towards the end, rather than pause at sharp end of cosine scheduler\n",
        "else:\n",
        "  switch_epoch = 100"
      ],
      "metadata": {
        "id": "Rk9z1gwoPfa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqWO8Edb0BK2"
      },
      "outputs": [],
      "source": [
        "best_valacc = 0\n",
        "\n",
        "for epoch in range(config[SELECT_MODEL]['epochs']):\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "    if not USE_CELOSS:\n",
        "      train_acc, train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    else:\n",
        "      # during training, we fix the learning rate for Celoss so not using any scheduler\n",
        "      train_acc, train_loss = train_celoss(\n",
        "          model, train_loader, optimizer, finetune_optimizer, \n",
        "          criterion, finetune_loss, config['celoss_weight'])\n",
        "    \n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        config[SELECT_MODEL]['epochs'],\n",
        "        train_acc,\n",
        "        train_loss,\n",
        "        curr_lr))\n",
        "    \n",
        "    val_acc, val_loss = validate(model, valid_loader, criterion)\n",
        "    # check if scheduler needs to be switched\n",
        "    scheduler.step()\n",
        "    if epoch == switch_epoch:\n",
        "      scheduler.step()\n",
        "      print(\"switching scheduler\")\n",
        "      if SELECT_MODEL == 'spherenet':\n",
        "        # for spherenet, only train for a few epoch, so use stepLR\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.6, step_size=1)\n",
        "      else:\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.6, patience=1, verbose=True)\n",
        "\n",
        "    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(val_acc, val_loss))\n",
        "\n",
        "    wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc, \n",
        "               'validation_loss': val_loss, \"learning_Rate\": curr_lr})\n",
        "    \n",
        "    # If you are using a scheduler in your train function within your iteration loop, you may want to log\n",
        "    # your learning rate differently \n",
        "\n",
        "    # #Save model in drive location if val_acc is better than best recorded val_acc\n",
        "    if val_acc >= best_valacc:\n",
        "      #path = os.path.join(root, model_directory, 'checkpoint' + '.pth')\n",
        "      print(\"Saving model at epoch \"+str(epoch+1))\n",
        "      save_vals = {'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optimizer.state_dict(),\n",
        "                  'scheduler_state_dict':scheduler.state_dict(),\n",
        "                  'val_acc': val_acc, \n",
        "                  'val_loss': val_loss,\n",
        "                  'learning_rate': curr_lr,\n",
        "                  'epoch': epoch}\n",
        "      if USE_CELOSS:\n",
        "        save_vals['celoss_state'] = finetune_loss.state_dict()\n",
        "        save_vals['celoss_optimizer'] = finetune_optimizer.state_dict()\n",
        "      \n",
        "\n",
        "      torch.save(save_vals, './checkpoint_spherenet_w_celoss.pth')\n",
        "      \n",
        "      best_valacc = val_acc\n",
        "      wandb.save('checkpoint_spherenet_w_celoss.pth')\n",
        "\n",
        "    gc.collect() # These commands help you when you face CUDA OOM error\n",
        "    torch.cuda.empty_cache()\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "\n",
        "# spherenet + cross loss > 96 batch\n",
        "# + celoss > 64 batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "id": "f8G-kNxUiidB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FineTuning"
      ],
      "metadata": {
        "id": "rA-DjI-g636_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bCmzKfH7XgG"
      },
      "outputs": [],
      "source": [
        "# add your finetune/retrain code here\n",
        "# use celoss_lr = 0.5, lambda = 0.003\n",
        "def reload_prev_model(modeltype, modelpath, scheduler, use_celoss=USE_CELOSS):\n",
        "  if modeltype == 'spherenet':\n",
        "    model = SphereNet36(config['num_classes']).to(DEVICE)\n",
        "    model.apply(init_weights)\n",
        "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.6, step_size=1)\n",
        "  elif modeltype == 'resnet':\n",
        "    model = ResNet34(config['num_classes']).to(DEVICE)\n",
        "    model.apply(init_weights)\n",
        "\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "  checkpoint = torch.load(modelpath)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "  print(\"Current Val ACC, LOSS, LR and EPOCH from reload is:\\n\")\n",
        "  print(checkpoint['val_acc'], checkpoint['learning_rate'], checkpoint['epoch'])\n",
        "\n",
        "  if use_celoss:\n",
        "      finetune_loss = CenterLoss(config['num_classes'], 512)\n",
        "      finetune_optimizer = torch.optim.SGD(finetune_loss.parameters(), lr = config['celoss_lr']) \n",
        "      finetune_loss.load_state_dict(checkpoint['celoss_state'])\n",
        "      finetune_optimizer.load_state_dict(checkpoint['celoss_optimizer'])\n",
        "      return model, optimizer, scheduler, finetune_loss, finetune_optimizer\n",
        "  else:\n",
        "    return model, optimizer, scheduler\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Level Finetune - 15 epoch for 2 times"
      ],
      "metadata": {
        "id": "D1UYIkKwKFeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelpath = './checkpoint_spherenet_w_celoss.pth'\n",
        "model, optimizer, scheduler = reload_prev_model('spherenet', modelpath, scheduler, False)\n",
        "# reseting learning rate for finetuning \n",
        "optimizer.param_groups[0]['lr'] = config['finetune']['model_lr']\n",
        "# for finetuning, load a new loss with smaller loss weight\n",
        "finetune_loss = CenterLoss(config['num_classes'], 512, config['finetune']['celoss_weight'])\n",
        "finetune_optimizer = torch.optim.SGD(finetune_loss.parameters(), lr = config['finetune']['celoss_lr']) \n",
        "# create scheduler for both model and loss with 15 epoch loop\n",
        "scheduler_1ft = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 15)\n",
        "finetune_scheduler_1ft = torch.optim.lr_scheduler.CosineAnnealingLR(finetune_optimizer, 15)\n",
        "\n",
        "print(optimizer.param_groups[0]['lr'])\n",
        "print(finetune_optimizer.param_groups[0]['lr'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0iANr4OUOQp",
        "outputId": "ddd6c565-8f3d-4d04-c121-419a8df358be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Val ACC, LOSS, LR and EPOCH from reload is:\n",
            "\n",
            "0.6798446069469836 0.15 0\n",
            "0.01\n",
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_valacc = 90\n",
        "\n",
        "# start finetuning for 15 epoch each 2 times\n",
        "for epoch in range(config['finetune']['epochs_1']):\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "    curr_losslr = float(finetune_optimizer.param_groups[0]['lr'])\n",
        "    train_acc, train_loss = train_celoss(\n",
        "          model, train_loader, optimizer, finetune_optimizer, \n",
        "          criterion, finetune_loss, config['finetune']['celoss_weight'])\n",
        "    \n",
        "    print(\"\\nFinetune Epoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.04f}, Loss LR {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        config['finetune']['epochs_1'],\n",
        "        train_acc,\n",
        "        train_loss,\n",
        "        curr_lr, \n",
        "        curr_losslr))\n",
        "    \n",
        "    val_acc, val_loss = validate(model, valid_loader, criterion)\n",
        "    scheduler_1ft.step()\n",
        "    finetune_scheduler_1ft.step()\n",
        "    \n",
        "    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(val_acc, val_loss))\n",
        "    \n",
        "    wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc, \n",
        "               'validation_loss': val_loss, \"learning_Rate\": curr_lr})\n",
        "    \n",
        "    # If you are using a scheduler in your train function within your iteration loop, you may want to log\n",
        "    # your learning rate differently \n",
        "\n",
        "    # #Save model in drive location if val_acc is better than best recorded val_acc\n",
        "    if val_acc >= best_valacc:\n",
        "      #path = os.path.join(root, model_directory, 'checkpoint' + '.pth')\n",
        "      print(\"Saving model at epoch \"+str(epoch+1))\n",
        "      save_vals = {'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optimizer.state_dict(),\n",
        "                  'scheduler_state_dict':scheduler_1ft.state_dict(),\n",
        "                  'val_acc': val_acc, \n",
        "                  'val_loss': val_loss,\n",
        "                  'learning_rate': curr_lr,\n",
        "                  'ft_learning_rate': curr_losslr, \n",
        "                  'epoch': epoch}\n",
        "      if USE_CELOSS:\n",
        "        save_vals['celoss_state'] = finetune_loss.state_dict()\n",
        "        save_vals['celoss_optimizer'] = finetune_optimizer.state_dict()\n",
        "      torch.save(save_vals, './checkpoint_spherenet_finetune.pth')\n",
        "      \n",
        "      best_valacc = val_acc\n",
        "      wandb.save('checkpoint_spherenet_finetune.pth')\n",
        "\n",
        "    gc.collect() # These commands help you when you face CUDA OOM error\n",
        "    torch.cuda.empty_cache()\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "#run.finish()"
      ],
      "metadata": {
        "id": "aeKMf1st7Mhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second Level Finetune - 10 epoch for 1 times"
      ],
      "metadata": {
        "id": "zPm15RogKfes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if FINETUNE_FROM_LOAD:\n",
        "  modelpath = './checkpoint_spherenet_finetune.pth'  \n",
        "  model, optimizer, scheduler, finetune_loss, finetune_optimizer = reload_prev_model('spherenet', modelpath, scheduler, True)\n",
        "\n",
        "# reseting learning rate for finetuning \n",
        "optimizer.param_groups[0]['lr'] = config['finetune']['model_lr']*config['finetune']['2ft_lr_factor']\n",
        "finetune_optimizer.param_groups[0]['lr'] = config['finetune']['celoss_lr']*config['finetune']['2ft_lr_factor']\n",
        "# create scheduler for both model and loss with 15 epoch loop\n",
        "scheduler_2ft = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10)\n",
        "finetune_scheduler_2ft = torch.optim.lr_scheduler.CosineAnnealingLR(finetune_optimizer, 10)\n",
        "\n",
        "print(optimizer.param_groups[0]['lr'])\n",
        "print(finetune_optimizer.param_groups[0]['lr'])"
      ],
      "metadata": {
        "id": "WdO94FDOhu6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43fa444-7a4b-4778-82df-13656b090ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001\n",
            "0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_valacc = 90\n",
        "\n",
        "# start finetuning for 10 epoch each 1 times\n",
        "for epoch in range(config['finetune']['epochs_2']):\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "    curr_losslr = float(finetune_optimizer.param_groups[0]['lr'])\n",
        "    train_acc, train_loss = train_celoss(\n",
        "          model, train_loader, optimizer, finetune_optimizer, \n",
        "          criterion, finetune_loss, config['finetune']['celoss_weight'])\n",
        "    \n",
        "    print(\"\\nFinetune Epoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.04f}, Loss LR {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        config['finetune']['epochs_2'],\n",
        "        train_acc,\n",
        "        train_loss,\n",
        "        curr_lr, \n",
        "        curr_losslr))\n",
        "    \n",
        "    val_acc, val_loss = validate(model, valid_loader, criterion)\n",
        "    scheduler_2ft.step()\n",
        "    finetune_scheduler_2ft.step()\n",
        "    \n",
        "    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(val_acc, val_loss))\n",
        "    \n",
        "    wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc, \n",
        "               'validation_loss': val_loss, \"learning_Rate\": curr_lr})\n",
        "    \n",
        "    # If you are using a scheduler in your train function within your iteration loop, you may want to log\n",
        "    # your learning rate differently \n",
        "\n",
        "    # #Save model in drive location if val_acc is better than best recorded val_acc\n",
        "    if val_acc >= best_valacc:\n",
        "      #path = os.path.join(root, model_directory, 'checkpoint' + '.pth')\n",
        "      print(\"Saving model at epoch \"+str(epoch+1))\n",
        "      save_vals = {'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optimizer.state_dict(),\n",
        "                  'scheduler_state_dict':scheduler_2ft.state_dict(),\n",
        "                  'val_acc': val_acc, \n",
        "                  'val_loss': val_loss,\n",
        "                  'learning_rate': curr_lr,\n",
        "                  'ft_learning_rate': curr_losslr, \n",
        "                  'epoch': epoch}\n",
        "      if USE_CELOSS:\n",
        "        save_vals['celoss_state'] = finetune_loss.state_dict()\n",
        "        save_vals['celoss_optimizer'] = finetune_optimizer.state_dict()\n",
        "      torch.save(save_vals, './checkpoint_spherenet_finetune_2.pth')\n",
        "      \n",
        "      best_valacc = val_acc\n",
        "      wandb.save('checkpoint_spherenet_finetune_2.pth')\n",
        "\n",
        "    gc.collect() # These commands help you when you face CUDA OOM error\n",
        "    torch.cuda.empty_cache()\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "EkF1pw2KMBJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpgCHImRkYQW"
      },
      "source": [
        "# Classification Task: Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2WQEUjXkWvo"
      },
      "outputs": [],
      "source": [
        "def test(model,dataloader):\n",
        "\n",
        "  model.eval()\n",
        "  batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "  test_results = []\n",
        "  \n",
        "  for i, (images) in enumerate(dataloader):\n",
        "      # TODO: Finish predicting on the test set.\n",
        "      images = images.to(DEVICE)\n",
        "\n",
        "      with torch.inference_mode():\n",
        "        outputs = model(images)\n",
        "\n",
        "      outputs = torch.argmax(outputs, axis=1).detach().cpu().numpy().tolist()\n",
        "      test_results.extend(outputs)\n",
        "      \n",
        "      batch_bar.update()\n",
        "      \n",
        "  batch_bar.close()\n",
        "  return test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7R1lcCAzULc"
      },
      "outputs": [],
      "source": [
        "test_results = test(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqfUzwS2L1gx"
      },
      "source": [
        "## Generate csv to submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vob9a2-HkW_V"
      },
      "outputs": [],
      "source": [
        "with open(\"classification_spherenet_submission_sub.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(test_dataset)):\n",
        "        f.write(\"{},{}\\n\".format(str(i).zfill(5) + \".jpg\", test_results[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnRUN53CZMTf"
      },
      "outputs": [],
      "source": [
        "if SUBMIT_KAGGLE:\n",
        "  !kaggle competitions submit -c 11-785-s23-hw2p2-classification -f classification_spherenet_submission_sub.csv -m \"submission\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WYgUjJzUiGU"
      },
      "source": [
        "# Verification Task: Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoBFFF8-Lpvj"
      },
      "source": [
        "The verification task consists of the following generalized scenario:\n",
        "- You are given X unknown identitites \n",
        "- You are given Y known identitites\n",
        "- Your goal is to match X unknown identities to Y known identities.\n",
        "\n",
        "We have given you a verification dataset, that consists of 960 known identities, and 1080 unknown identities. The 1080 unknown identities are split into dev (360) and test (720). Your goal is to compare the unknown identities to the 1080 known identities and assign an identity to each image from the set of unknown identities. Some unknown identities do not have correspondence in known identities, you also need to identify these and label them with a special label n000000.\n",
        "\n",
        "Your will use/finetune your model trained for classification to compare images between known and unknown identities using a similarity metric and assign labels to the unknown identities. \n",
        "\n",
        "This will judge your model's performance in terms of the quality of embeddings/features it generates on images/faces it has never seen during training for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9aY5o-suWdn"
      },
      "outputs": [],
      "source": [
        "# This obtains the list of known identities from the known folder\n",
        "known_regex = \"/content/data/11-785-s23-hw2p2-verification/known/*/*\"\n",
        "known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))]\n",
        "\n",
        "# Obtain a list of images from unknown folders\n",
        "unknown_dev_regex = \"/content/data/11-785-s23-hw2p2-verification/unknown_dev/*\"\n",
        "unknown_test_regex = \"/content/data/11-785-s23-hw2p2-verification/unknown_test/*\"\n",
        "\n",
        "# We load the images from known and unknown folders\n",
        "unknown_dev_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_dev_regex)))]\n",
        "unknown_test_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_test_regex)))]\n",
        "known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]\n",
        "\n",
        "# Why do you need only ToTensor() here?\n",
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "unknown_dev_images = torch.stack([transforms(x) for x in unknown_dev_images])\n",
        "unknown_test_images = torch.stack([transforms(x) for x in unknown_test_images])\n",
        "known_images  = torch.stack([transforms(y) for y in known_images ])\n",
        "#Print your shapes here to understand what we have done\n",
        "\n",
        "# You can use other similarity metrics like Euclidean Distance if you wish\n",
        "similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk1LS0BRxFHM"
      },
      "outputs": [],
      "source": [
        "def eval_verification(unknown_images, known_images, model, similarity, batch_size= config['batch_size'], mode='val'): \n",
        "\n",
        "    unknown_feats, known_feats = [], []\n",
        "\n",
        "    batch_bar = tqdm(total=len(unknown_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
        "    model.eval()\n",
        "\n",
        "    # We load the images as batches for memory optimization and avoiding CUDA OOM errors\n",
        "    for i in range(0, unknown_images.shape[0], batch_size):\n",
        "        unknown_batch = unknown_images[i:i+batch_size] # Slice a given portion upto batch_size\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            #unknown_feat = model(unknown_batch.float().to(DEVICE), return_feats=True) #Get features from model         \n",
        "            unknown_feat, unknown_out = model(unknown_batch.float().to(DEVICE), return_feats=True) #Get features from model         \n",
        "        \n",
        "        unknown_feats.append(unknown_feat)\n",
        "        batch_bar.update()\n",
        "    \n",
        "    batch_bar.close()\n",
        "    \n",
        "    batch_bar = tqdm(total=len(known_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
        "    \n",
        "    for i in range(0, known_images.shape[0], batch_size):\n",
        "        known_batch = known_images[i:i+batch_size] \n",
        "        with torch.no_grad():\n",
        "              #known_feat = model(known_batch.float().to(DEVICE), return_feats=True)\n",
        "              known_feat, known_out = model(known_batch.float().to(DEVICE), return_feats=True)\n",
        "          \n",
        "        known_feats.append(known_feat)\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    # Concatenate all the batches\n",
        "    unknown_feats = torch.cat(unknown_feats, dim=0)\n",
        "    known_feats = torch.cat(known_feats, dim=0)\n",
        "\n",
        "    similarity_values = torch.stack([similarity(unknown_feats, known_feature) for known_feature in known_feats])\n",
        "    # Print the inner list comprehension in a separate cell - what is really happening?\n",
        "    #print(similarity_values.shape)\n",
        "    #print(similarity_values)\n",
        "    max_similarity_values, predictions = similarity_values.max(0) #Why are we doing an max here, where are the return values?\n",
        "    max_similarity_values, predictions = max_similarity_values.cpu().numpy(), predictions.cpu().numpy()\n",
        "    print(max_similarity_values.mean())\n",
        "\n",
        "    # Note that in unknown identities, there are identities without correspondence in known identities.\n",
        "    # Therefore, these identities should be not similar to all the known identities, i.e. max similarity will be below a certain \n",
        "    # threshold compared with those identities with correspondence.\n",
        "\n",
        "    # In early submission, you can ignore identities without correspondence, simply taking identity with max similarity value\n",
        "    # pred_id_strings = [known_paths[i] for i in predictions] # Map argmax indices to identity strings\n",
        "    \n",
        "    # After early submission, remove the previous line and uncomment the following code \n",
        "\n",
        "    NO_CORRESPONDENCE_LABEL = 'n000000'\n",
        "    pred_id_strings = []\n",
        "    for idx, prediction in enumerate(predictions):\n",
        "        if max_similarity_values[idx] <  0.435: # 0.388 why < ? Thank about what is your similarity metric\n",
        "             pred_id_strings.append(NO_CORRESPONDENCE_LABEL)\n",
        "        else:\n",
        "             pred_id_strings.append(known_paths[prediction])\n",
        "    \n",
        "    if mode == 'val':\n",
        "      true_ids = pd.read_csv('/content/data/11-785-s23-hw2p2-verification/verification_dev.csv')['label'].tolist()\n",
        "      accuracy = accuracy_score(pred_id_strings, true_ids)\n",
        "      print(\"Verification Accuracy = {}\".format(accuracy))\n",
        "    \n",
        "    return pred_id_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMC7FacaUnJ7"
      },
      "outputs": [],
      "source": [
        "# verification eval - finetune 2\n",
        "#0.6694444 by 0.42, 0.675 by 0.45, 0.6805556 by 0.475, 0.683333 by 0.5, 0.6861111 by 0.525 AND 535 and 512\n",
        "#0.68888889 BY 0.515\n",
        "# finetune - 3\n",
        "# 0.6666 by 0.505, 0.697 by 0.475, 0.702777 by 0.465, 0.71111 by 0.45, 0.708333 by 0.435, 0.7138888 by 0.445\n",
        "# 0.71111 by 0.442\n",
        "pred_id_strings = eval_verification(unknown_dev_images, known_images, model, similarity_metric, config['batch_size'], mode='val')\n",
        "# verification test\n",
        "pred_id_strings = eval_verification(unknown_test_images, known_images, model, similarity_metric, config['batch_size'], mode='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTLW0RPD7XGC"
      },
      "source": [
        "## Generate csv to submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD-r-HmsAeWV"
      },
      "outputs": [],
      "source": [
        "with open(\"verification_spherenet_submission_adjthreshold.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(pred_id_strings)):\n",
        "        f.write(\"{},{}\\n\".format(i, pred_id_strings[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPIgq0tMZ8qk"
      },
      "outputs": [],
      "source": [
        "if SUBMIT_KAGGLE:\n",
        "  !kaggle competitions submit -c 11-785-s23-hw2p2-verification -f verification_spherenet_submission_adjthreshold.csv -m \"early submission\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "scOnMklwWBY6",
        "sSeiKHYrM-6b",
        "mIqmojPaWD0H",
        "kYZAPgHlr2SW",
        "K-sOpBHQ6-Vh",
        "dzM11HtcboYv",
        "2mBgKGkXLrdJ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}